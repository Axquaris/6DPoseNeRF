<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>6D iNeRF</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<span class="logo"><img src="images/logo.svg" alt="" /></span>
						<h1>Pose Estimation with Neural Radiance Fields</h1>
						<p>Domas Buracas, Nir Levin, Gregory Jerian, Matthew Harrigan<br>CS184</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Abstract</a></li>
							<li><a href="#second">Technical Approach</a></li>
							<li><a href="#first">Results</a></li>
							<li><a href="#cta">References</a></li>
							<li><a href="#contrib">Individual Contributions</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							<section id="video" class="main special">
								<header class="major">
									<h2>Final Video</h2>
									<iframe width="560" height="315" src="https://www.youtube.com/embed/zq72I35PUNw"
											title="YouTube video player" frameborder="0" allow="accelerometer; autoplay;
											clipboard-write; encrypted-media; gyroscope; picture-in-picture"
											allowfullscreen></iframe>
								</header>
								<footer class="major">
								</footer>
							</section>

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Abstract</h2>
										</header>
										<p>We have re-implemented the approach to 6D pose estimation
											introduced in [iNeRF] by optimizing a 6D pose to minimize
											the difference between pixels rendered at a “guess” pose
											and a target image. This effectively inverts [NeRF] to recover
											the pose where the target image could have been rendered or photographed from.
											<br><br>
											In addition, we’ve created many different ways to visualize the process
											and results, including the random sampling distribution, movies showing
											the optimization process, and 3D graphs of the loss surfaces.</p>
									</div>
									<span class="image"><img src="images/pic01.jpg" alt="" /></span>
								</div>
							</section>

						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>Technical Approach</h2>
								</header>
								<p class="content">
									The first thing we did was load NeRF so we could render images from various viewpoints.
									Unfortunately, NeRF took 11 seconds on average per render, which was not very fast.
									To speed up the rendering process, we imported the svox library, which contains an
									implementation of N3 trees, an octree-like data structure that allowed us to render
									the images much more quickly. After incorporating svox, our renders sped up to
									approximately 0 seconds.
								</p>
								<p class="content">
									Next, we wrote our pose optimization loop, as inspired by the iNeRF paper.
									The purpose of this loop is to optimize our guessed parameters so that they
									are as equal as possible to those of the target image. To accomplish this,
									our loop calculates a forward pass on a transformation input and backpropagates
									on the loss between this calculated image and the target one. Using gradient
									descent, we are able to adjust the values of our input parameters accordingly
									to minimize the loss of our target image.

								</p>
								<p class="content">
									One of the biggest problems we ran into when implementing the pose optimization
									loop was that raycasting to every single part of the image was too computationally
									expensive to do for every single pass. Because of this, we had to implement a
									pixel sampling scheme to choose specific pixels to sample, calculate the rays
									from the camera to those pixels on the image plane, and generate an image from
									these rays. When considering sampling schemes, we tried to choose a scheme that was
									computationally inexpensive while still representing the image well, so that we
									would only have a minor decrease in accuracy while maintaining a large speed up.

								</p>

								<p class="content">
									Some examples of sampling techniques we tried out are random sampling and
									Canny sampling. Random sampling was consistently outperformed by Canny sampling,
									which places a greater weight on edges. This is probably due to random sampling
									often picking parts of the background, which is completely white representative of
									the pose of the actual image. Because Canny sampling was
									able to perform better and faster, leading to faster convergence of our
									parameters, we ended up keeping it for the final product.
								</p>

								<p class="content">
									We cycled through several different methods of parameterizing the camera pose in
									order to determine which method is the most efficient and effective. At first,
									we started off trying to optimize the 4x4 transformation matrix, but we realized
									that this representation of a transformation was too general. This is because
									transformation matrices need to be orthogonal, but the optimization loop doesn't
									take this factor into account; each element of the matrix is tuned on its own.
									Thus, we needed a representation of transformations where each parameter could
									truly be any value within a certain range.
								</p>

								<p class="content">
									We decided to constrain this transformation matrix. One thing we experienced with the
									transformation matrix parameterization is shearing. Since the objects in the
									dataset are never sheared, only rotated, this is definitely an unnecessary
									endeavour, so we seeked to remove the possibility of shearing in our pose
									parameterization. For this, we turned to the parameterization that was used in the
									iNeRF paper: exponential coordinates consisting of a twist vector and angle.
									The twist vector representation is 6 dimensional, and stores information about the
									axis vector the twist is happening about, as well as translation along that vector.
									The angle specifies the amount of rotation around the axis. This parameterization
									already does much better than the transformation.

								</p>

								<p class="content">
									We can constrain this parameterization even further by locking rotations to just
									different axes (pitch and yaw) instead of a general rotation axis at any angle.
									This was by far the best performing parameterization, as it works well even with
									very few samples of the image generated by the forward pass.
								</p>

								<p class="content">
									From this project, we learned many important lessons. First, we learned that
									picking an overly ambitious project can be a lot more work than any of us were
									expecting. Even though we completed most of what we wished to accomplish, we ended
									up spending a lot of time on this. On the technical side, we learned that
									the parameters we use to represent the object we are optimizing are just as
									important as the optimization procedure itself, and we had to eventually change
									our representation to our twist vector plus thetas one in order optimization
									loop to properly work. Additionally, we learned that in order to speed up our
									computationally intensive algorithm, we can
									be clever about selecting a subset of our data and making the problem much faster
									while retaining the important data. In this project, this was our sampling scheme,
									however this is generally a good lesson to apply to advanced algorithms in many
									fields.
								</p>
							</section>

							<!-- Third Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Results</h2>
								</header>

								<p class="content">
									Here are some examples of our pose optimization loop being optimized on our image
									with random sampling. Notice that the model has trouble optimizing when our
									starting pose is very different from the target pose:
								</p>
								<ul class="features">
									<li>
										<img src="images/rand1.gif" align="right" width="400px" />
										<p>Random sampling with the starting pose being very similar to the target pose.</p>
									</li>
									<li>
										<img src="images/rand2.gif" align="right" width="400px" />
										<p>Random sampling with a moderate difference from the target pose.</p>
									</li>
									<li>
										<img src="images/rand3.gif" align="right" width="400px" />
										<p>Random sampling with a large difference from the target pose.</p>
									</li>
								</ul>

								<p class="content">
									Here are some examples of our pose optimization loop being optimized on our
									image with Canny sampling:
								</p>

								<ul class="features">
									<li>
										<img src="images/canny1.gif" align="right" width="400px" />
										<p>Canny sampling with the starting pose being very similar to the target pose.</p>
									</li>
									<li>
										<img src="images/canny2.gif" align="right" width="400px" />
										<p>Canny sampling with a moderate difference from the target pose.</p>
									</li>
									<li>
										<img src="images/canny3.gif" align="right" width="400px" />
										<p>Canny sampling with a large difference from the target pose.</p>
									</li>
								</ul>



								<p class="content">
									Here are some interesting results. (show results with fewer pixels, or
									1 pixel, or that crazy flipping shit)
								</p>

							</section>

						<!-- Fourth Section -->
							<section id="cta" class="main special">
								<header class="major">
									<h2>References</h2>
								</header>
								<p class="content">
									<ul >
										<li>
											NeRF [https://www.matthewtancik.com/nerf]
										</li>
										<li>
											iNeRF [https://yenchenlin.me/inerf/]
										</li>
									</ul>
								</p>

							</section>

						<!-- Fifth Section -->
							<section id="contrib" class="main special">
								<header class="major">
									<h2>Individual Contributions</h2>
								</header>
								<p class="content">
									This project was Domas’s brainchild, who researched nerf and inerf and pitched the
									project to the rest of us. Domas was involved in nearly every aspect of the project and
									was always around to offer advice. He worked most heavily on pose optimization as well
									as generating all the results and testing everything, since svox only worked on his
									machine.
									<br>
									Matthew came up with and implemented the various sampling algorithms, including Canny
									and high-importance sampling, as well experimented with numerous other image
									pre-processing and sampling techniques. Matthew also worked on the websites.
									<br>
									Greg worked on getting NeRF and svox working, ironing out the intricacies of the
									libraries. He worked on rendering and pixel sampling of the model. He also narrated,
									edited, and published the videos.
									<br>
									Nir worked on the pose optimization loop, including the forward and backward passes
									and pose parameterizations. Nir and Domas often worked in tandem. Nir put together our
									websites and translated our google docs to html.
								</p>
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>