<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>6D iNeRF</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<h1>Pose Estimation with Neural Radiance Fields</h1>
						<p>Domas Buracas, Nir Levin, Gregory Jerian, Matthew Harrigan<br>CS184</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
                            <li><a href="#video">Video</a></li>
							<li><a href="#intro" class="active">Abstract</a></li>
							<li><a href="#second">Technical Approach</a></li>
							<li><a href="#first">Results</a></li>
							<li><a href="#cta">References</a></li>
							<li><a href="#contrib">Individual Contributions</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							<section id="video" class="main special">
								<header class="major">
									<h2>Video</h2>
									<iframe width="560" height="315" src="https://www.youtube.com/embed/zq72I35PUNw"
											title="YouTube video player" frameborder="0" allow="accelerometer; autoplay;
											clipboard-write; encrypted-media; gyroscope; picture-in-picture"
											allowfullscreen></iframe>
								</header>
								<footer class="major">
								</footer>
							</section>

						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Abstract</h2>
										</header>
										<p>We have re-implemented the approach to 6D pose estimation
											introduced in <a href=https://yenchenlin.me/inerf>iNeRF</a> by optimizing a 6D pose to minimize
											the difference between pixels rendered at a “guess” pose
											and a target image. This effectively inverts <a href=https://www.matthewtancik.com/nerf>NeRF</a> to recover
											the pose where the target image could have been rendered or photographed from.
											<br><br>
											In addition, we’ve created many different ways to visualize the process
											and results, including the random sampling distribution, movies showing
											the optimization process, and 3D graphs of the loss surfaces.</p>
									</div>
									<span ><img src="images/parameterization.gif" alt="" /></span>
								</div>
							</section>

						<!-- Second Section -->
							<section id="second" class="main special">
								<header class="major">
									<h2>Technical Approach</h2>
								</header>
								<p class="content">
									The first thing we did was load NeRF so we could render images from various viewpoints.
									Unfortunately, NeRF took 11 seconds on average per render, which was not very fast.
									To speed up the rendering process, we imported the svox library, which contains an
									implementation of N3 trees, an octree-like data structure that allowed us to render
									the images much more quickly. After incorporating svox, our renders sped up to
									approximately 0 seconds.
								</p>
								<p class="content">
									Next, we wrote our pose optimization loop, as inspired by the iNeRF paper.
									The purpose of this loop is to optimize our guessed parameters so that they
									are as close as possible to those of the target image. To accomplish this,
									our loop calculates a forward pass on a transformation input and backpropagates
									on the loss between this calculated image and the target one. Using gradient
									descent, we are able to adjust the values of our input parameters accordingly
									to minimize the loss of our target image.

								</p>
								<p class="content">
									One of the biggest problems we ran into when implementing the pose optimization
									loop was that raycasting to every single part of the image was too computationally
									expensive to do for every single pass. Because of this, we had to implement a
									pixel sampling scheme to choose specific pixels to sample, calculate the rays
									from the camera to those pixels on the image plane, and generate an image from
									these rays. When considering sampling schemes, we tried to choose a scheme that was
									computationally inexpensive while still representing the image well, so that we
									would only have a minor decrease in accuracy while maintaining a large speed up.

								</p>

								<p class="content">
									Some examples of sampling techniques we tried out are random sampling and
									Canny sampling. Random sampling was consistently outperformed by Canny sampling,
									which places a greater weight on edges. This is probably due to random sampling
									often picking parts of the background, which is completely white and not representative of
									the pose of the actual image. Because Canny sampling was
									able to perform better and faster, leading to faster convergence of our
									parameters, we ended up keeping it for the final product.
								</p>

								<ul class="features">
									<li>
										<img src="images/random_sampling.png" class="centeredimg" width="400px" />
										<p>Random sampling.</p>
									</li>
									<li>
										<img src="images/canny_sampling.png" class="centeredimg" width="400px" />
										<p>Canny sampling.</p>
									</li>
								</ul>

								<p class="content">
									We cycled through several different methods of parameterizing the camera pose in
									order to determine which method is the most efficient and effective. At first,
									we started off trying to optimize the 4x4 transformation matrix, but we realized
									that this representation of a transformation was too general. This is because
									transformation matrices need to be orthogonal, but the optimization loop doesn't
									take this factor into account; each element of the matrix is tuned on its own.
									Thus, we needed a representation of transformations where each parameter could
									truly be any value within a certain range.
								</p>

								<p class="content">
									We decided to constrain this transformation matrix. One thing we experienced with the
									transformation matrix parameterization is shearing. Since the objects in the
									dataset are never sheared, only rotated, we wanted to remove the possibility of shearing in our pose
									parameterization. For this, we turned to the parameterization that was used in the
									iNeRF paper: exponential coordinates consisting of a twist vector and angle.
									The twist vector representation is 6-dimensional, storing information about the
									axis vector the twist is happening about as well as translation along that vector.
									The angle specifies the amount of rotation around the axis.
								</p>

								<p class="content">
									We can constrain this parameterization even further by locking rotations to just
									different axes (pitch and yaw) instead of a general rotation axis at any angle.
									This was by far the best performing parameterization, as it works well even with
									very few samples of the image generated by the forward pass.
								</p>

                                <ul class="features">
									<li>
										<img src="images/loss_euler.png" class="centeredimg" width="400px" />
										<p>Loss vs. epoch for Euler angle parameterization.</p>
									</li>
									<li>
										<img src="images/loss_twist.png" class="centeredimg" width="400px" />
										<p>Loss vs. epoch for twist vector parameterization.</p>
									</li>
									<li>
										<img src="images/loss_transformation.png" class="centeredimg" width="400px" />
										<p>Loss vs. epoch for transformation matrix parameterization.</p>
									</li>
								</ul>

								<p class="content">
									From this project, we learned many important lessons. First, we learned a lot about
                  working as a team, creating deadlines for ourselves, and following a schedule.
                  On the technical side, we learned that the parameters we use to represent the object 
                  we are optimizing are just as
									important as the optimization procedure itself, and we had to eventually change
									our representation as described above to get our optimization
									loop to properly work. Additionally, we learned that in order to speed up our
									computationally intensive algorithm, we could
									be clever about selecting a subset of our data. This made the algorithm much faster
									while still retaining the important data. In this project, this was our sampling scheme,
									however this is generally a good lesson to apply to advanced algorithms in many
									fields.
								</p>
							</section>

							<!-- Third Section -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Results</h2>
								</header>

                <h3>Twist Parameterization</h2>
								<p class="content">
									Here are some examples of our pose optimization loop being optimized on our image
									with random sampling. Notice that the model has trouble optimizing when our
									starting pose is very different from the target pose:
								</p>
								<ul class="features">
									<li>
										<img src="images/rand1.gif" class="centeredimg" width="400px" />
										<p>Random sampling with a small difference from the target pose.</p>
									</li>
									<li>
										<img src="images/rand2.gif" class="centeredimg" width="400px" />
										<p>Random sampling with a moderate difference from the target pose.</p>
									</li>
									<li>
										<img src="images/rand3.gif" class="centeredimg" width="400px" />
										<p>Random sampling with a large difference from the target pose.</p>
									</li>
								</ul>

								<p class="content">
									Here are some examples of our pose optimization loop being optimized on our
									image with Canny sampling:
								</p>

								<ul class="features">
									<li>
										<img src="images/canny1.gif" class="centeredimg" width="400px" />
										<p>Canny sampling with a small difference from the target pose.</p>
									</li>
									<li>
										<img src="images/canny2.gif" class="centeredimg" width="400px" />
										<p>Canny sampling with a moderate difference from the target pose.</p>
									</li>
									<li>
										<img src="images/canny3.gif" class="centeredimg" width="400px" />
										<p>Canny sampling with a large difference from the target pose.</p>
									</li>
								</ul>

                <h3>Euler Parameterization</h2>
                <p class="content">
                  Here are some examples of our pose optimization loop being optimized Using
                  euler parameterization:
                </p>

                <ul class="features">
                  <li>
                    <img src="images/euler1.gif" class="centeredimg" width="400px" />
                    <p>Canny sampling with a small difference from the target pose.</p>
                  </li>
                  <li>
                    <img src="images/euler2.gif" class="centeredimg" width="400px" />
                    <p>Canny sampling with a moderate difference from the target pose.</p>
                  </li>
                  <li>
                    <img src="images/euler3.gif" class="centeredimg" width="400px" />
                    <p>Canny sampling with a large difference from the target pose.</p>
                  </li>
                </ul>

							</section>

						<!-- Fourth Section -->
						<section id="cta" class="main special">
							<header class="major">
								<h2>References</h2>
							</header>
              <ul class="features">
                <div class="underlineless">
                  <a href=https://www.matthewtancik.com/nerf>
                    <span class="icon major style3 fa-copy"></span>
                    <br>
                    Original NeRF paper
                  </a>
                </div>
                <div class="underlineless">
                  <a href=https://svox.readthedocs.io/en/latest>
                    <span class="icon solid major style1 fa-code"></span>
                    <br>
                    svox library
                  </a>
                </div>
                <div class="underlineless">
                  <a href=https://yenchenlin.me/inerf>
                    <span class="icon major style3 fa-copy"></span>
                    <br>
                    Original iNeRF paper
                  </a>
                </div>
              </ul>

						</section>

						<!-- Fifth Section -->
						<section id="contrib" class="main special">
							<header class="major">
								<h2>Individual Contributions</h2>
							</header>
							<p class="content">
								This project was Domas’s brainchild, who researched NeRF and iNeRF and pitched the
								project to the rest of us. Domas was involved in nearly every aspect of the project and
								was always around to offer advice. He worked most heavily on pose optimization as well
								as generating all the results and testing everything, since svox only worked on his
								machine.
							</p>
							<p class="content">
								Matthew came up with and implemented the various sampling algorithms, including Canny
								and high-importance sampling, as well as experimenting with numerous other image
								pre-processing and sampling techniques. Matthew also worked on the websites.
							</p>
							<p class="content">
								Greg worked on getting NeRF and svox working, ironing out the intricacies of the
								libraries, as well as rendering and pixel sampling of the model. He also narrated,
								edited, and published the videos and worked on the websites.
							</p>
							<p class="content">
								Nir worked on the pose optimization loop, including the forward and backward passes
								and pose parameterizations. Nir and Domas often worked in tandem. Nir put together our
								websites and translated our Google Docs to HTML.
							</p>
						</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>